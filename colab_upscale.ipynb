{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400b5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ğŸ“¦ Cell 1: å®‰è£…ä¾èµ– + æŒ‚è½½ Google Drive\n",
    "!pip install spandrel -q\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆï¼ŒDrive å·²æŒ‚è½½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title âš™ï¸ Cell 2: é…ç½®å‚æ•°\n",
    "import os\n",
    "\n",
    "# ============ æ¨¡å‹é…ç½® ============\n",
    "MODEL_URL = \"https://huggingface.co/datasets/Kizi-Art/Upscale/resolve/main/4x-UltraSharp.pth\"\n",
    "MODEL_DIR = \"/content/models\"\n",
    "MODEL_NAME = \"4x-UltraSharp.pth\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_NAME)\n",
    "\n",
    "# ============ æ”¾å¤§é…ç½® ============\n",
    "TARGET_SCALE = 8          #@param {type:\"slider\", min:4, max:16, step:4}\n",
    "MAX_SIDE_LENGTH = 8192    #@param {type:\"slider\", min:4096, max:16384, step:1024}\n",
    "TILE_SIZE = 512           #@param {type:\"slider\", min:128, max:1024, step:128}\n",
    "TILE_OVERLAP = 32         # åˆ†å—é‡å åƒç´ \n",
    "\n",
    "# ============ è·¯å¾„é…ç½® (ç›´æ¥ç”¨ Drive) ============\n",
    "DRIVE_BASE = \"/content/drive/MyDrive/upscale\"  #@param {type:\"string\"}\n",
    "INPUT_DIR = f\"{DRIVE_BASE}/input\"\n",
    "OUTPUT_DIR = f\"{DRIVE_BASE}/output\"\n",
    "\n",
    "# åˆ›å»ºç›®å½•\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ è¾“å…¥ç›®å½•: {INPUT_DIR}\")\n",
    "print(f\"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}\")\n",
    "print(f\"ğŸ¯ ç›®æ ‡æ”¾å¤§å€æ•°: {TARGET_SCALE}x\")\n",
    "\n",
    "print(f\"ğŸ§© åˆ†å—å¤§å°: {TILE_SIZE}\")\n",
    "print(\"\\nğŸ’¡ æç¤º: æŠŠå›¾ç‰‡æ”¾åˆ° Drive çš„ upscale/input æ–‡ä»¶å¤¹ï¼Œç„¶åè¿è¡Œåç»­ Cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3816d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ğŸ“‚ Cell 3: æ£€æŸ¥è¾“å…¥æ–‡ä»¶å¤¹\n",
    "import glob\n",
    "\n",
    "extensions = ['*.png', '*.jpg', '*.jpeg', '*.webp', '*.bmp']\n",
    "image_files = []\n",
    "for ext in extensions:\n",
    "    image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n",
    "    image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper())))\n",
    "\n",
    "if not image_files:\n",
    "    print(f\"âŒ è¾“å…¥æ–‡ä»¶å¤¹ä¸ºç©º: {INPUT_DIR}\")\n",
    "    print(\"ğŸ’¡ è¯·å…ˆæŠŠå›¾ç‰‡æ”¾åˆ° Google Drive çš„ upscale/input æ–‡ä»¶å¤¹\")\n",
    "else:\n",
    "    print(f\"âœ… æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡:\")\n",
    "\n",
    "    for f in image_files:\n",
    "        print(f\"  ğŸ“· {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251df689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ğŸ“¥ Cell 4: ä¸‹è½½æ¨¡å‹\n",
    "import os\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½ {MODEL_NAME}...\")\n",
    "    !wget -q -O \"{MODEL_PATH}\" \"{MODEL_URL}\"\n",
    "    print(f\"âœ… æ¨¡å‹å·²ä¸‹è½½åˆ° {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"âœ… æ¨¡å‹å·²å­˜åœ¨: {MODEL_PATH}\")\n",
    "\n",
    "# æ£€æŸ¥æ–‡ä»¶å¤§å°\n",
    "size_mb = os.path.getsize(MODEL_PATH) / (1024 * 1024)\n",
    "print(f\"ğŸ“¦ æ¨¡å‹å¤§å°: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ğŸ”§ Cell 5: åŠ è½½æ¨¡å‹\n",
    "import torch\n",
    "import spandrel\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...\")\n",
    "\n",
    "# æ£€æŸ¥ CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model_descriptor = spandrel.ModelLoader(device=device).load_from_file(MODEL_PATH)\n",
    "model = model_descriptor.model\n",
    "model.half()  # FP16 èŠ‚çœæ˜¾å­˜\n",
    "model.eval()\n",
    "\n",
    "MODEL_SCALE = model_descriptor.scale  # æ¨¡å‹åŸç”Ÿæ”¾å¤§å€æ•° (4x)\n",
    "\n",
    "print(f\"âœ… æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "print(f\"ğŸ“ æ¨¡å‹åŸç”Ÿå€æ•°: {MODEL_SCALE}x\")\n",
    "print(f\"ğŸ¯ ç›®æ ‡å€æ•°: {TARGET_SCALE}x â†’ éœ€è¦è¿­ä»£ {TARGET_SCALE // MODEL_SCALE} æ¬¡\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1be492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ğŸ› ï¸ Cell 6: å®šä¹‰å¤„ç†å‡½æ•°\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "def pil_to_torch_bgr(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"PIL Image (RGB) â†’ Torch Tensor (NCHW, BGR, FP16, 0~1)\"\"\"\n",
    "    img_np = np.array(img).astype(np.float32) / 255.0\n",
    "    # RGB â†’ BGR\n",
    "    if img_np.ndim == 3 and img_np.shape[2] == 3:\n",
    "        img_np = img_np[:, :, ::-1].copy()  # copy() è§£å†³è´Ÿ stride é—®é¢˜\n",
    "    # HWC â†’ CHW â†’ NCHW\n",
    "    img_tensor = torch.from_numpy(img_np.transpose(2, 0, 1).copy()).unsqueeze(0)\n",
    "    return img_tensor.half().to(device)\n",
    "\n",
    "def torch_bgr_to_pil(tensor: torch.Tensor) -> Image.Image:\n",
    "    \"\"\"Torch Tensor (NCHW, BGR) â†’ PIL Image (RGB)\"\"\"\n",
    "    img_np = tensor.squeeze(0).float().cpu().clamp(0, 1).numpy()\n",
    "    # CHW â†’ HWC\n",
    "    img_np = img_np.transpose(1, 2, 0)\n",
    "    # BGR â†’ RGB\n",
    "    img_np = img_np[:, :, ::-1].copy()  # copy() è§£å†³è´Ÿ stride é—®é¢˜\n",
    "    img_np = (img_np * 255).astype(np.uint8)\n",
    "    return Image.fromarray(img_np)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def tiled_upscale(img: Image.Image, tile_size: int = 512, overlap: int = 32) -> Image.Image:\n",
    "    \"\"\"\n",
    "    åˆ†å—æ”¾å¤§ï¼Œé¿å…æ˜¾å­˜çˆ†ç‚¸\n",
    "    å‚è€ƒ: stable-diffusion-webui/modules/upscaler_utils.py\n",
    "    \"\"\"\n",
    "    width, height = img.size\n",
    "    \n",
    "    # è®¡ç®—åˆ†å—æ•°é‡\n",
    "    tiles_x = math.ceil(width / (tile_size - overlap))\n",
    "    tiles_y = math.ceil(height / (tile_size - overlap))\n",
    "    \n",
    "    # è¾“å‡ºå°ºå¯¸\n",
    "    out_width = width * MODEL_SCALE\n",
    "    out_height = height * MODEL_SCALE\n",
    "    output = Image.new('RGB', (out_width, out_height))\n",
    "    \n",
    "    # æ”¾å¤§åçš„ overlap\n",
    "    scaled_overlap = overlap * MODEL_SCALE\n",
    "    \n",
    "    for y in range(tiles_y):\n",
    "        for x in range(tiles_x):\n",
    "            # è®¡ç®—è¾“å…¥ tile çš„åæ ‡\n",
    "            x1 = x * (tile_size - overlap)\n",
    "            y1 = y * (tile_size - overlap)\n",
    "            x2 = min(x1 + tile_size, width)\n",
    "            y2 = min(y1 + tile_size, height)\n",
    "            \n",
    "            # ç¡®ä¿ tile å°ºå¯¸ä¸€è‡´ï¼ˆè¾¹ç¼˜æƒ…å†µï¼‰\n",
    "            if x2 - x1 < tile_size and x1 > 0:\n",
    "                x1 = max(0, x2 - tile_size)\n",
    "            if y2 - y1 < tile_size and y1 > 0:\n",
    "                y1 = max(0, y2 - tile_size)\n",
    "            \n",
    "            # è£åˆ‡ tile\n",
    "            tile = img.crop((x1, y1, x2, y2))\n",
    "            \n",
    "            # æ”¾å¤§\n",
    "            tile_tensor = pil_to_torch_bgr(tile)\n",
    "            upscaled_tensor = model(tile_tensor)\n",
    "            upscaled_tile = torch_bgr_to_pil(upscaled_tensor)\n",
    "            \n",
    "            # è®¡ç®—è¾“å‡ºåæ ‡\n",
    "            out_x1 = x1 * MODEL_SCALE\n",
    "            out_y1 = y1 * MODEL_SCALE\n",
    "            out_x2 = x2 * MODEL_SCALE\n",
    "            out_y2 = y2 * MODEL_SCALE\n",
    "            \n",
    "            # è®¡ç®—æœ‰æ•ˆåŒºåŸŸï¼ˆå»é™¤é‡å éƒ¨åˆ†ï¼Œé™¤äº†è¾¹ç¼˜ï¼‰\n",
    "            paste_x1 = 0 if x == 0 else scaled_overlap // 2\n",
    "            paste_y1 = 0 if y == 0 else scaled_overlap // 2\n",
    "            paste_x2 = upscaled_tile.width if x == tiles_x - 1 else upscaled_tile.width - scaled_overlap // 2\n",
    "            paste_y2 = upscaled_tile.height if y == tiles_y - 1 else upscaled_tile.height - scaled_overlap // 2\n",
    "            \n",
    "            # è£åˆ‡æœ‰æ•ˆåŒºåŸŸ\n",
    "            valid_tile = upscaled_tile.crop((paste_x1, paste_y1, paste_x2, paste_y2))\n",
    "            \n",
    "            # ç²˜è´´åˆ°è¾“å‡º\n",
    "            final_x = out_x1 + paste_x1\n",
    "            final_y = out_y1 + paste_y1\n",
    "            output.paste(valid_tile, (final_x, final_y))\n",
    "    \n",
    "    return output\n",
    "\n",
    "def multi_scale_upscale(img: Image.Image, target_scale: int) -> Image.Image:\n",
    "    \"\"\"\n",
    "    æ”¾å¤§å›¾ç‰‡åˆ°ç›®æ ‡å€æ•°ï¼Œä¸è¶…è¿‡ MAX_SIDE_LENGTH\n",
    "    å‚è€ƒ: stable-diffusion-webui Extras é€»è¾‘\n",
    "    \"\"\"\n",
    "    orig_w, orig_h = img.size\n",
    "    max_orig_side = max(orig_w, orig_h)\n",
    "    \n",
    "    # è®¡ç®—å®é™…å¯ç”¨çš„æ”¾å¤§å€æ•°ï¼ˆä¸è¶…è¿‡ MAX_SIDE_LENGTHï¼‰\n",
    "    max_possible_scale = MAX_SIDE_LENGTH / max_orig_side\n",
    "    actual_scale = min(target_scale, max_possible_scale)\n",
    "    \n",
    "    # è®¡ç®—ç›®æ ‡å°ºå¯¸\n",
    "    target_w = int(orig_w * actual_scale)\n",
    "    target_h = int(orig_h * actual_scale)\n",
    "    \n",
    "    print(f\"  ğŸ¯ ç›®æ ‡å°ºå¯¸: {target_w}x{target_h} ({actual_scale:.2f}x)\")\n",
    "    \n",
    "    # è®¡ç®—éœ€è¦å¤šå°‘æ¬¡ ESRGAN æ”¾å¤§æ‰èƒ½ >= actual_scale\n",
    "    iterations = 0\n",
    "    esrgan_scale = 1\n",
    "    while esrgan_scale < actual_scale:\n",
    "\n",
    "        iterations += 1print(\"âœ… å¤„ç†å‡½æ•°å·²å®šä¹‰\")\n",
    "\n",
    "        esrgan_scale *= MODEL_SCALE\n",
    "\n",
    "        return current\n",
    "\n",
    "    # å¦‚æœ iterations = 0 è¯´æ˜åŸå›¾å·²ç»å¤Ÿå¤§    \n",
    "\n",
    "    if iterations == 0:        current = current.resize((target_w, target_h), Image.LANCZOS)\n",
    "\n",
    "        print(f\"  âš ï¸ åŸå›¾å·²æ¥è¿‘æœ€å¤§è¾¹é•¿ï¼Œæ— éœ€æ”¾å¤§\")        print(f\"  ğŸ”„ Resize: {current.size[0]}x{current.size[1]} â†’ {target_w}x{target_h}\")\n",
    "\n",
    "        return img    if current.size != (target_w, target_h):\n",
    "\n",
    "        # æœ€å resize åˆ°ç²¾ç¡®çš„ç›®æ ‡å°ºå¯¸ï¼ˆå› ä¸º ESRGAN åªèƒ½ 4x å€æ•°ï¼‰\n",
    "\n",
    "    # ESRGAN è¿­ä»£æ”¾å¤§    \n",
    "\n",
    "    current = img        current = tiled_upscale(current, TILE_SIZE, TILE_OVERLAP)\n",
    "\n",
    "    for i in range(iterations):        print(f\"  ğŸ“ è¿­ä»£ {i+1}/{iterations}: {current.size[0]}x{current.size[1]} â†’ {current.size[0]*MODEL_SCALE}x{current.size[1]*MODEL_SCALE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca0d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ğŸš€ Cell 7: æ‰¹é‡å¤„ç† â†’ ä¿å­˜åˆ° Drive\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# è·å–æ‰€æœ‰å›¾ç‰‡\n",
    "extensions = ['*.png', '*.jpg', '*.jpeg', '*.webp', '*.bmp']\n",
    "image_files = []\n",
    "for ext in extensions:\n",
    "    image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext)))\n",
    "    image_files.extend(glob.glob(os.path.join(INPUT_DIR, ext.upper())))\n",
    "\n",
    "if not image_files:\n",
    "    print(\"âŒ æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡ï¼Œè¯·å…ˆè¿è¡Œ Cell 3 ä¸Šä¼ å›¾ç‰‡\")\n",
    "else:\n",
    "    print(f\"ğŸ“· æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡\")\n",
    "    print(f\"ğŸ¯ ç›®æ ‡å€æ•°: {TARGET_SCALE}x\")\n",
    "    print(f\"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, img_path in enumerate(image_files, 1):\n",
    "        filename = os.path.basename(img_path)\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(image_files)}] å¤„ç†: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # åŠ è½½å›¾ç‰‡\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            orig_size = img.size\n",
    "            print(f\"  ğŸ“ åŸå§‹å°ºå¯¸: {orig_size[0]}x{orig_size[1]}\")\n",
    "            \n",
    "            # æ”¾å¤§\n",
    "            upscaled = multi_scale_upscale(img, TARGET_SCALE)\n",
    "            final_size = upscaled.size\n",
    "            print(f\"  âœ… æœ€ç»ˆå°ºå¯¸: {final_size[0]}x{final_size[1]}\")\n",
    "            \n",
    "            # ä¿å­˜ (ä¿ç•™åŸæ–‡ä»¶å + å®é™…å°ºå¯¸)\n",
    "            output_filename = f\"{name}_{final_size[0]}x{final_size[1]}{ext}\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "            upscaled.save(output_path, quality=95)\n",
    "            print(f\"  ğŸ’¾ å·²ä¿å­˜: {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ é”™è¯¯: {e}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"ğŸ‰ å…¨éƒ¨å®Œæˆ! è€—æ—¶: {elapsed:.1f} ç§’\")\n",
    "    print(f\"ğŸ“ è¾“å‡ºä½ç½®: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ğŸ”Œ Cell 8: æ–­å¼€è¿è¡Œæ—¶\n",
    "from google.colab import runtime\n",
    "\n",
    "print(\"âš ï¸ å³å°†æ–­å¼€è¿è¡Œæ—¶...\")\n",
    "print(\"ğŸ“ è¾“å‡ºå·²ä¿å­˜åˆ° Google Driveï¼Œæ— éœ€ç­‰å¾…ä¸‹è½½\")\n",
    "\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
