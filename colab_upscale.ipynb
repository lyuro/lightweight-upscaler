{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168953ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ– (Colab é€šå¸¸å·²æœ‰ torch/PIL)\n",
    "!pip install spandrel -q\n",
    "!pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac21f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "\n",
    "# ============== é…ç½®åŒº ==============\n",
    "# æ¨¡å‹è·¯å¾„ (è‡ªåŠ¨ä¸‹è½½ 4x-UltraSharp)\n",
    "MODEL_URL = \"https://huggingface.co/datasets/Kizi-Art/Upscale/resolve/main/4x-UltraSharp.pth\"\n",
    "MODEL_PATH = \"/content/models/4x-UltraSharp.pth\"\n",
    "\n",
    "# è¾“å…¥è¾“å‡ºç›®å½•\n",
    "INPUT_DIR = \"/content/input\"   # æŠŠå›¾ç‰‡æ”¾è¿™é‡Œ\n",
    "OUTPUT_DIR = \"/content/output\" # è¾“å‡ºåˆ°è¿™é‡Œ\n",
    "\n",
    "# æ”¾å¤§è®¾ç½®\n",
    "TARGET_SCALE = 8             # ç›®æ ‡æ”¾å¤§å€æ•° (8 æˆ– 16)\n",
    "MAX_SIDE_LENGTH = 8192       # æœ€å¤§è¾¹é•¿é™åˆ¶\n",
    "TILE_SIZE = 512              # åˆ†å—å¤§å° (æ˜¾å­˜ä¸è¶³å¯é™ä½)\n",
    "TILE_OVERLAP = 32            # åˆ†å—é‡å \n",
    "\n",
    "# è®¾å¤‡\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "USE_HALF = True              # ä½¿ç”¨åŠç²¾åº¦ (çœæ˜¾å­˜)\n",
    "\n",
    "print(f\"ä½¿ç”¨è®¾å¤‡: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb6d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½æ¨¡å‹\n",
    "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"æ­£åœ¨ä¸‹è½½ 4x-UltraSharp æ¨¡å‹...\")\n",
    "    !wget -q -O \"{MODEL_PATH}\" \"{MODEL_URL}\"\n",
    "    print(\"ä¸‹è½½å®Œæˆ!\")\n",
    "else:\n",
    "    print(\"æ¨¡å‹å·²å­˜åœ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¸å¿ƒå‡½æ•°: å›¾åƒè½¬æ¢\n",
    "def pil_to_torch_bgr(img: Image.Image) -> torch.Tensor:\n",
    "    \"\"\"PIL RGB -> Torch BGR [0,1]\"\"\"\n",
    "    arr = np.array(img.convert(\"RGB\"))\n",
    "    arr = arr[:, :, ::-1]  # RGB -> BGR\n",
    "    arr = np.transpose(arr, (2, 0, 1))  # HWC -> CHW\n",
    "    arr = np.ascontiguousarray(arr) / 255.0\n",
    "    return torch.from_numpy(arr).float()\n",
    "\n",
    "def torch_bgr_to_pil(tensor: torch.Tensor) -> Image.Image:\n",
    "    \"\"\"Torch BGR [0,1] -> PIL RGB\"\"\"\n",
    "    if tensor.ndim == 4:\n",
    "        tensor = tensor.squeeze(0)\n",
    "    arr = tensor.float().cpu().clamp_(0, 1).numpy()\n",
    "    arr = 255.0 * np.moveaxis(arr, 0, 2)  # CHW -> HWC\n",
    "    arr = arr.round().astype(np.uint8)\n",
    "    arr = arr[:, :, ::-1]  # BGR -> RGB\n",
    "    return Image.fromarray(arr, \"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dddb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ ¸å¿ƒå‡½æ•°: åˆ†å—æ”¾å¤§ (é¿å…æ˜¾å­˜çˆ†ç‚¸)\n",
    "def tiled_upscale(\n",
    "    img: Image.Image,\n",
    "    model,\n",
    "    tile_size: int = 512,\n",
    "    tile_overlap: int = 32,\n",
    "    scale: int = 4,\n",
    "    device: str = \"cuda\",\n",
    "    dtype = torch.float16\n",
    ") -> Image.Image:\n",
    "    \"\"\"åˆ†å—æ”¾å¤§å›¾åƒ\"\"\"\n",
    "    w, h = img.size\n",
    "    \n",
    "    # å¦‚æœå›¾åƒå°äº tile_sizeï¼Œç›´æ¥æ•´å¼ å¤„ç†\n",
    "    if w <= tile_size and h <= tile_size:\n",
    "        tensor = pil_to_torch_bgr(img).unsqueeze(0).to(device=device, dtype=dtype)\n",
    "        with torch.inference_mode():\n",
    "            output = model(tensor)\n",
    "        return torch_bgr_to_pil(output)\n",
    "    \n",
    "    # åˆ†å—å¤„ç†\n",
    "    stride = tile_size - tile_overlap\n",
    "    h_tiles = list(range(0, h - tile_size, stride)) + [max(0, h - tile_size)]\n",
    "    w_tiles = list(range(0, w - tile_size, stride)) + [max(0, w - tile_size)]\n",
    "    \n",
    "    # è¾“å‡ºç”»å¸ƒ\n",
    "    out_h, out_w = h * scale, w * scale\n",
    "    result = torch.zeros((3, out_h, out_w), dtype=torch.float32)\n",
    "    weight = torch.zeros((1, out_h, out_w), dtype=torch.float32)\n",
    "    \n",
    "    total_tiles = len(h_tiles) * len(w_tiles)\n",
    "    \n",
    "    with tqdm(total=total_tiles, desc=\"åˆ†å—æ”¾å¤§\", leave=False) as pbar:\n",
    "        for y in h_tiles:\n",
    "            for x in w_tiles:\n",
    "                # è£å‰ª tile\n",
    "                tile = img.crop((x, y, x + tile_size, y + tile_size))\n",
    "                tensor = pil_to_torch_bgr(tile).unsqueeze(0).to(device=device, dtype=dtype)\n",
    "                \n",
    "                with torch.inference_mode():\n",
    "                    output = model(tensor).squeeze(0).float().cpu()\n",
    "                \n",
    "                # è®¡ç®—è¾“å‡ºä½ç½®\n",
    "                out_y, out_x = y * scale, x * scale\n",
    "                out_tile_h, out_tile_w = output.shape[1], output.shape[2]\n",
    "                \n",
    "                # ç´¯åŠ ç»“æœ (é‡å åŒºåŸŸä¼šè¢«å¹³å‡)\n",
    "                result[:, out_y:out_y+out_tile_h, out_x:out_x+out_tile_w] += output\n",
    "                weight[:, out_y:out_y+out_tile_h, out_x:out_x+out_tile_w] += 1\n",
    "                \n",
    "                pbar.update(1)\n",
    "    \n",
    "    # å¹³å‡é‡å åŒºåŸŸ\n",
    "    result = result / weight.clamp(min=1)\n",
    "    \n",
    "    return torch_bgr_to_pil(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ¨¡å‹\n",
    "import spandrel\n",
    "\n",
    "print(\"æ­£åœ¨åŠ è½½æ¨¡å‹...\")\n",
    "model_descriptor = spandrel.ModelLoader(device=DEVICE).load_from_file(MODEL_PATH)\n",
    "model = model_descriptor.model\n",
    "\n",
    "if USE_HALF and model_descriptor.supports_half:\n",
    "    model.half()\n",
    "    print(\"ä½¿ç”¨ FP16 åŠç²¾åº¦\")\n",
    "\n",
    "model.eval()\n",
    "MODEL_SCALE = model_descriptor.scale  # é€šå¸¸æ˜¯ 4\n",
    "print(f\"æ¨¡å‹åŠ è½½å®Œæˆ! å•æ¬¡æ”¾å¤§å€æ•°: {MODEL_SCALE}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šæ¬¡è¿­ä»£æ”¾å¤§\n",
    "def multi_scale_upscale(\n",
    "    img: Image.Image,\n",
    "    model,\n",
    "    target_scale: float,\n",
    "    model_scale: int = 4,\n",
    "    max_side: int = 8192,\n",
    "    tile_size: int = 512,\n",
    "    tile_overlap: int = 32,\n",
    "    device: str = \"cuda\"\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    å¤šæ¬¡è¿­ä»£æ”¾å¤§åˆ°ç›®æ ‡å€æ•°\n",
    "    ä¾‹å¦‚: 8x = 4x + 4x (ä¸¤æ¬¡), 16x = 4x + 4x + 4x + 4x (å››æ¬¡)\n",
    "    \"\"\"\n",
    "    dtype = torch.float16 if USE_HALF else torch.float32\n",
    "    current_img = img\n",
    "    current_scale = 1.0\n",
    "    \n",
    "    iteration = 0\n",
    "    while current_scale < target_scale:\n",
    "        iteration += 1\n",
    "        w, h = current_img.size\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦ä¼šè¶…è¿‡æœ€å¤§è¾¹é•¿\n",
    "        next_w, next_h = w * model_scale, h * model_scale\n",
    "        if max(next_w, next_h) > max_side:\n",
    "            print(f\"  è­¦å‘Š: ä¸‹ä¸€æ¬¡æ”¾å¤§ä¼šè¶…è¿‡ {max_side}pxï¼Œåœæ­¢è¿­ä»£\")\n",
    "            break\n",
    "        \n",
    "        print(f\"  è¿­ä»£ {iteration}: {w}x{h} -> {next_w}x{next_h}\")\n",
    "        \n",
    "        current_img = tiled_upscale(\n",
    "            current_img,\n",
    "            model,\n",
    "            tile_size=tile_size,\n",
    "            tile_overlap=tile_overlap,\n",
    "            scale=model_scale,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "        \n",
    "        current_scale *= model_scale\n",
    "        \n",
    "        # æ¸…ç†æ˜¾å­˜\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return current_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹é‡å¤„ç†\n",
    "def batch_upscale(\n",
    "    input_dir: str,\n",
    "    output_dir: str,\n",
    "    model,\n",
    "    target_scale: float = 8,\n",
    "    max_side: int = 8192,\n",
    "    tile_size: int = 512,\n",
    "    tile_overlap: int = 32\n",
    "):\n",
    "    \"\"\"æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰å›¾ç‰‡\"\"\"\n",
    "    supported_formats = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\"}\n",
    "    \n",
    "    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶\n",
    "    input_path = Path(input_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    image_files = [\n",
    "        f for f in input_path.iterdir()\n",
    "        if f.suffix.lower() in supported_formats\n",
    "    ]\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"åœ¨ {input_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶\")\n",
    "        return\n",
    "    \n",
    "    print(f\"æ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡\")\n",
    "    print(f\"ç›®æ ‡æ”¾å¤§å€æ•°: {target_scale}x, æœ€å¤§è¾¹é•¿: {max_side}px\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i, img_path in enumerate(image_files, 1):\n",
    "        print(f\"\\n[{i}/{len(image_files)}] å¤„ç†: {img_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            # åŠ è½½å›¾ç‰‡\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            orig_w, orig_h = img.size\n",
    "            print(f\"  åŸå§‹å°ºå¯¸: {orig_w}x{orig_h}\")\n",
    "            \n",
    "            # æ”¾å¤§\n",
    "            result = multi_scale_upscale(\n",
    "                img,\n",
    "                model,\n",
    "                target_scale=target_scale,\n",
    "                model_scale=MODEL_SCALE,\n",
    "                max_side=max_side,\n",
    "                tile_size=tile_size,\n",
    "                tile_overlap=tile_overlap,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            \n",
    "            # ä¿å­˜ (ä¿ç•™åŸæ–‡ä»¶å)\n",
    "            output_file = output_path / f\"{img_path.stem}_upscaled.png\"\n",
    "            result.save(output_file, \"PNG\")\n",
    "            \n",
    "            final_w, final_h = result.size\n",
    "            print(f\"  æœ€ç»ˆå°ºå¯¸: {final_w}x{final_h}\")\n",
    "            print(f\"  å·²ä¿å­˜: {output_file.name}\")\n",
    "            \n",
    "            # æ¸…ç†\n",
    "            del img, result\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  é”™è¯¯: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"å¤„ç†å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸Šä¼ å›¾ç‰‡åˆ° input ç›®å½• (Colab)\n",
    "from google.colab import files\n",
    "\n",
    "print(\"è¯·ä¸Šä¼ è¦æ”¾å¤§çš„å›¾ç‰‡...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename, data in uploaded.items():\n",
    "    with open(os.path.join(INPUT_DIR, filename), 'wb') as f:\n",
    "        f.write(data)\n",
    "    print(f\"å·²ä¿å­˜: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fade2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹æ‰¹é‡æ”¾å¤§!\n",
    "batch_upscale(\n",
    "    input_dir=INPUT_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    model=model,\n",
    "    target_scale=TARGET_SCALE,\n",
    "    max_side=MAX_SIDE_LENGTH,\n",
    "    tile_size=TILE_SIZE,\n",
    "    tile_overlap=TILE_OVERLAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½ç»“æœ (å¯é€‰)\n",
    "import shutil\n",
    "\n",
    "# æ‰“åŒ…è¾“å‡ºç›®å½•\n",
    "shutil.make_archive(\"/content/upscaled_images\", 'zip', OUTPUT_DIR)\n",
    "files.download(\"/content/upscaled_images.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d5444",
   "metadata": {},
   "source": [
    "## ğŸ“ è¯´æ˜\n",
    "\n",
    "**ä¾èµ–æç®€**:\n",
    "- `spandrel`: ç”¨äºåŠ è½½ ESRGAN ç±»æ¨¡å‹ (æ›¿ä»£å®Œæ•´ SD WebUI)\n",
    "- `torch`, `PIL`, `numpy`: åŸºç¡€ä¾èµ–\n",
    "\n",
    "**ä¼˜åŠ¿**:\n",
    "1. æ— éœ€åŠ è½½ Stable Diffusion æ¨¡å‹ (çœæ˜¾å­˜/æ—¶é—´)\n",
    "2. ç›´æ¥ä½¿ç”¨ spandrel åŠ è½½ .pth æ¨¡å‹\n",
    "3. æ”¯æŒåˆ†å—å¤„ç† (tile) é¿å…æ˜¾å­˜çˆ†ç‚¸\n",
    "4. å¤šæ¬¡è¿­ä»£æ”¾å¤§æ”¯æŒ 8x/16x\n",
    "5. ä¿ç•™åŸæ–‡ä»¶å\n",
    "\n",
    "**æ˜¾å­˜å»ºè®®**:\n",
    "- T4 (16GB): TILE_SIZE=512 å¯ç”¨\n",
    "- V100/A100: TILE_SIZE=768 æ›´å¿«\n",
    "- æ˜¾å­˜ä¸è¶³: é™ä½ TILE_SIZE åˆ° 256"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
